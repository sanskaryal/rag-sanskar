{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aa857dd0",
      "metadata": {
        "id": "aa857dd0"
      },
      "source": [
        "# RAG Sanskar Notebook\n",
        "\n",
        "This notebook demonstrates a basic Retrieval-Augmented Generation (RAG) system setup using the `google-genai` library (Gemini LLM) together with text chunking and simple keyword-based retrieval. The notebook uses code from both the `rag_system.py` file and `requirements.txt` for dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46e57b0b",
      "metadata": {
        "id": "46e57b0b"
      },
      "outputs": [],
      "source": [
        "# Uncomment the next line to install dependencies directly from the notebook\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2fd5e7b",
      "metadata": {
        "id": "d2fd5e7b"
      },
      "source": [
        "## 2. Import Libraries and Define Functions\n",
        "\n",
        "We define functions for splitting text into chunks with overlap and for retrieving the most relevant chunks based on a simple keyword matching scheme. This setup is similar to what is in your `rag_system.py` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "be6f563d",
      "metadata": {
        "id": "be6f563d",
        "outputId": "1e74b8f1-07d7-430a-8596-af0e95fafe17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:502: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "# Import the Gemini client from google-genai\n",
        "from google import genai\n",
        "\n",
        "def chunk_text(text, chunk_size=600, overlap=100):\n",
        "    \"\"\"\n",
        "    Splits the given text into overlapping chunks using a sliding window approach.\n",
        "    - text: Input string to be splitted.\n",
        "    - chunk_size: The length of each chunk.\n",
        "    - overlap: The number of characters overlapping between consecutive chunks.\n",
        "    \"\"\"\n",
        "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size - overlap)]\n",
        "\n",
        "def find_relevant_chunks(query, chunks, top_k=3):\n",
        "    \"\"\"\n",
        "    Finds and returns the indices of the chunks that have the highest keyword overlap with the query.\n",
        "    This is a simple method to score relevance based on matching words.\n",
        "    \"\"\"\n",
        "    # Tokenize query into words (converted to lower case for uniformity)\n",
        "    query_words = set(re.findall(r'\\w+', query.lower()))\n",
        "    scores = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        # Tokenize chunk into words\n",
        "        chunk_words = set(re.findall(r'\\w+', chunk.lower()))\n",
        "        # Score is the count of common words between query and chunk\n",
        "        score = len(query_words & chunk_words)\n",
        "        scores.append((score, i))\n",
        "\n",
        "    # Sort chunks by score in descending order and get indices of the top scoring chunks\n",
        "    return [i for _, i in sorted(scores, reverse=True)[:top_k]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9067f22",
      "metadata": {
        "id": "f9067f22"
      },
      "source": [
        "## 3. Prepare Document and Process Text\n",
        "\n",
        "Here we define a sample document (a whimsical story) and perform basic text cleaning by removing extra spaces. We then create chunks from this cleaned text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "af2fed09",
      "metadata": {
        "id": "af2fed09",
        "outputId": "97ad18dd-3e5c-4588-dbcc-a8b5d6f8ed02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 2 chunks:\n",
            "['Once upon a time, in a whimsical land called Veggieville, there lived a curious rabbit named Bunny. Bunny was no ordinary rabbit—she had a knack for finding strange and magical objects. One sunny morning, while hopping through the Enchanted Forest, she stumbled upon a peculiar hat lying under a giant carrot-shaped tree. The hat was no ordinary hat—it was the Magic Hat of 2000 Lines, a legendary artifact said to grant its wearer the power to weave stories, spells, and songs with just a thought. Bunny picked up the hat and examined it closely. It was a tall, floppy hat with shimmering silver thr', 'unny picked up the hat and examined it closely. It was a tall, floppy hat with shimmering silver threads that seemed to dance in the sunlight. As she placed it on her head, a voice boomed from nowhere and everywhere at once. \"Ah, a new wearer! Welcome, Bunny. I am the Hat, and I am bound to serve you. But beware—my magic is not infinite. I can only create 2000 lines of magic before my power fades. Use them wisely!\" Bunny\\'s ears perked up. \"2000 lines? That\\'s a lot! What can I do with them?\" The Hat chuckled. \"Anything you can imagine! Poems, riddles, spells, even entire stories. But remember, ']\n"
          ]
        }
      ],
      "source": [
        "# Define the sample document\n",
        "document = \"\"\"\n",
        "Once upon a time, in a whimsical land called Veggieville, there lived a curious rabbit named Bunny. Bunny was no ordinary rabbit—she had a knack for finding strange and magical objects. One sunny morning, while hopping through the Enchanted Forest, she stumbled upon a peculiar hat lying under a giant carrot-shaped tree. The hat was no ordinary hat—it was the Magic Hat of 2000 Lines, a legendary artifact said to grant its wearer the power to weave stories, spells, and songs with just a thought.\n",
        "Bunny picked up the hat and examined it closely. It was a tall, floppy hat with shimmering silver threads that seemed to dance in the sunlight. As she placed it on her head, a voice boomed from nowhere and everywhere at once.\n",
        "\"Ah, a new wearer! Welcome, Bunny. I am the Hat, and I am bound to serve you. But beware—my magic is not infinite. I can only create 2000 lines of magic before my power fades. Use them wisely!\"\n",
        "Bunny's ears perked up. \"2000 lines? That's a lot! What can I do with them?\"\n",
        "The Hat chuckled. \"Anything you can imagine! Poems, riddles, spells, even entire stories. But remember, once the lines are used up, my magic is gone forever.\"\n",
        "Excited, Bunny decided to test the Hat's powers. She thought of her best friend, Carrot, a cheerful orange vegetable with a knack for getting into trouble. \"Hat, can you tell me a story about Carrot?\"\n",
        "The Hat glowed, and a story began to unfold:\n",
        "Once, in the heart of Veggieville, there lived a carrot named Carrot who loved to explore. One day, he stumbled upon a salty cave guarded by a grumpy old grain named Salt. Salt was the keeper of the Cave of Crystals, a place filled with shimmering treasures. But Salt was lonely and bitter, and he refused to let anyone enter.\n",
        "Carrot, being the curious soul he was, decided to befriend Salt. He brought him a basket of fresh vegetables and sang him a song so sweet that even Salt's hard exterior began to melt. Slowly, Salt opened up and shared his treasures with Carrot, and the two became the unlikeliest of friends.\n",
        "Bunny clapped her paws. \"That was amazing! But... how many lines did that use?\"\n",
        "The Hat sighed. \"That was 15 lines. You have 1985 left.\"\n",
        "Bunny gasped. \"Oh no! I need to be more careful. I don’t want to waste your magic.\"\n",
        "\"\"\"\n",
        "\n",
        "# Clean the text by replacing multiple whitespace with a single space\n",
        "clean_text = re.sub('\\s+', ' ', document).strip()\n",
        "\n",
        "# Split the cleaned text into chunks using the sliding window approach\n",
        "chunks = chunk_text(clean_text)\n",
        "\n",
        "# Print the first few chunks for verification\n",
        "print(\"First 2 chunks:\")\n",
        "print(chunks[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43a3e042",
      "metadata": {
        "id": "43a3e042"
      },
      "source": [
        "## 4. Set Up the Gemini Client and Execute a Query\n",
        "\n",
        "In this step, we set up the Gemini LLM client using an API key stored as an environment variable. Then we perform a simple retrieval-based generation:\n",
        "\n",
        "- We ask the user for a query.\n",
        "- We retrieve the top relevant chunks using our keyword matching function.\n",
        "- We then pass the retrieved context and query to Gemini and print the LLM's answer.\n",
        "\n",
        "> **Note:** In a notebook, interactive input might not work as expected. For demonstration purposes, you can set a sample query string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "060e3a6d",
      "metadata": {
        "id": "060e3a6d",
        "outputId": "66e0388d-bc16-4cc0-b9a9-50c69d7159ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dotenv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5021bbe6d53f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set up the Gemini client using the API key from an environment variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Ensure that you have set GEMINI_API_KEY in your environment for this cell to work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Set up the Gemini client using the API key from an environment variable\n",
        "# Ensure that you have set GEMINI_API_KEY in your environment for this cell to work\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "client = genai.Client(userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# For demonstration, we define a sample query.\n",
        "sample_query = \"What magic does the Hat have?\"\n",
        "\n",
        "# 1. Retrieve the indices of the most relevant chunks based on the sample query\n",
        "indices = find_relevant_chunks(sample_query, chunks)\n",
        "\n",
        "# 2. Print the relevant chunks\n",
        "print(\"\\nRelevant chunks:\")\n",
        "for i in indices:\n",
        "    print(f\"Chunk {i}: {chunks[i]}\")\n",
        "\n",
        "# 3. Create a single context string from the retrieved chunks\n",
        "context = \" \".join([chunks[i] for i in indices])\n",
        "\n",
        "# 4. Build the prompt for Gemini (including context and question)\n",
        "prompt = f\"Context: {context}\\nQuestion: {sample_query}\\nAnswer:\"\n",
        "\n",
        "# 5. Use Gemini to generate an answer based on the prompt\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "# 6. Print the answer\n",
        "print(\"\\nAnswer:\", response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf2b0d04",
      "metadata": {
        "id": "cf2b0d04"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook we:\n",
        "\n",
        "- **Imported required libraries** and set up helper functions for text chunking and retrieval.\n",
        "- **Cleaned and split a sample document** into overlapping chunks.\n",
        "- **Used a custom Gemini LLM client setup** to generate an answer from a sample query based on the retrieved context.\n",
        "\n",
        "This simple example illustrates how to combine retrieval techniques with LLM-powered generation in a notebook environment while providing full documentation and inline comments."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}