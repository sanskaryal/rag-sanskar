{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "aa857dd0",
            "metadata": {},
            "source": [
                "# Enhanced RAG System with Embeddings and FAISS\n",
                "\n",
                "This notebook demonstrates an improved RAG system using sentence embeddings and FAISS for efficient similarity search, along with Gemini for generation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "46e57b0b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required dependencies\n",
                "%pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d2fd5e7b",
            "metadata": {},
            "source": [
                "## 2. Import Libraries and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "be6f563d",
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import os\n",
                "import numpy as np\n",
                "import faiss\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from google import genai\n",
                "\n",
                "# Initialize embedding model\n",
                "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "\n",
                "def chunk_text(text, chunk_size=1000, overlap=200):\n",
                "    \"\"\"Split text into overlapping chunks using sliding window\"\"\"\n",
                "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size - overlap)]\n",
                "\n",
                "def create_faiss_index(embeddings):\n",
                "    \"\"\"Create FAISS index for efficient similarity search\"\"\"\n",
                "    dimension = embeddings.shape[1]\n",
                "    index = faiss.IndexFlatL2(dimension)\n",
                "    index.add(embeddings.astype(np.float32))\n",
                "    return index"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f9067f22",
            "metadata": {},
            "source": [
                "## 3. Prepare Document and Generate Embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 88,
            "id": "af2fed09",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Document fetched and processed.\n",
                        "Document chunks: 324\n",
                        "First 1 chunk example: [\"Alice’s Adventures in Wonderland | Project Gutenberg The Project Gutenberg eBook of Alice's Adventures in Wonderland This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org . If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. Title : Alice's Adventures in Wonderlan\"]\n",
                        "Embedding dimensions: (324, 384)\n"
                    ]
                }
            ],
            "source": [
                "# Sample document (same as previous)\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "\n",
                "# Fetch the HTML content from the URL\n",
                "url = \"https://www.gutenberg.org/cache/epub/11/pg11-images.html\"\n",
                "response = requests.get(url)\n",
                "html_content = response.text\n",
                "\n",
                "# Use BeautifulSoup to extract the textual content\n",
                "soup = BeautifulSoup(html_content, 'html.parser')\n",
                "for script in soup([\"script\", \"style\"]):\n",
                "    script.decompose()\n",
                "\n",
                "# Extract text and optionally limit length for performance\n",
                "document = soup.get_text(separator=\" \", strip=True)\n",
                "# Uncomment the next line to limit text (if needed)\n",
                "# document = document[:10000]\n",
                "\n",
                "print(\"Document fetched and processed.\")\n",
                "\n",
                "# Preprocess text\n",
                "clean_text = re.sub('\\s+', ' ', document).strip()\n",
                "\n",
                "# Create chunks\n",
                "chunks = chunk_text(clean_text)\n",
                "print(f\"Document chunks: {len(chunks)}\")\n",
                "print(f\"First 1 chunk example: {chunks[0:1]}\")\n",
                "\n",
                "# Generate embeddings\n",
                "chunk_embeddings = embedding_model.encode(chunks)\n",
                "print(f\"Embedding dimensions: {chunk_embeddings.shape}\")\n",
                "# print(f\"FAISS index dimensions: {chunk_embeddings[4]}\")\n",
                "\n",
                "# Create FAISS index\n",
                "index = create_faiss_index(chunk_embeddings)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "43a3e042",
            "metadata": {},
            "source": [
                "## 4. Query Processing and Retrieval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 89,
            "id": "060e3a6d",
            "metadata": {},
            "outputs": [],
            "source": [
                "def semantic_retrieval(query, index, chunks, top_k=3):\n",
                "    \"\"\"Retrieve relevant chunks using semantic similarity\"\"\"\n",
                "    # Encode query\n",
                "    query_embedding = embedding_model.encode([query])\n",
                "    \n",
                "    # Search FAISS index\n",
                "    indices = index.search(query_embedding.astype(np.float32), top_k)\n",
                "    print(f\"Indices: {indices[0]}\")\n",
                "    \n",
                "    # Return sorted chunks by relevance\n",
                "    return [chunks[i] for i in indices[0]]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cf2b0d04",
            "metadata": {},
            "source": [
                "## 5. Enhanced RAG Workflow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 99,
            "id": "060e3a6e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Indices: [205 177 255] [1.0280211 1.0399594 1.0416878]\n",
                        "Retrieved Context:\n",
                        "-  the middle of her favourite word ‘moral,’ and the arm that was linked into hers began to tremble. Alice looked up, and there stood the Queen in front of them, with her arms folded, frowning like a thunderstorm. “A fine day, your Majesty!” the Duchess began in a low, weak voice. “Now, I give you fair warning,” shouted the Queen, stamping on the ground as she spoke; “either you or your head must be off, and that in about half no time! Take your choice!” The Duchess took her choice, and was gone in a moment. “Let’s go on with the game,” the Queen said to Alice; and Alice was too much frightened ...\n",
                        "- sense!” said Alice, very loudly and decidedly, and the Queen was silent. The King laid his hand upon her arm, and timidly said “Consider, my dear: she is only a child!” The Queen turned angrily away from him, and said to the Knave “Turn them over!” The Knave did so, very carefully, with one foot. “Get up!” said the Queen, in a shrill, loud voice, and the three gardeners instantly jumped up, and began bowing to the King, the Queen, the royal children, and everybody else. “Leave off that!” screamed the Queen. “You make me giddy.” And then, turning to the rose-tree, she went on, “What have you be...\n",
                        "-  the Dormouse: “not in that ridiculous fashion.” And he got up very sulkily and crossed over to the other side of the court. All this time the Queen had never left off staring at the Hatter, and, just as the Dormouse crossed the court, she said to one of the officers of the court, “Bring me the list of the singers in the last concert!” on which the wretched Hatter trembled so, that he shook both his shoes off. “Give your evidence,” the King repeated angrily, “or I’ll have you executed, whether you’re nervous or not.” “I’m a poor man, your Majesty,” the Hatter began, in a trembling voice, “—and...\n",
                        "\n",
                        "Generated Answer:\n",
                        "Based on the context provided, the Queen shouted the following:\n",
                        "\n",
                        "*   \"Now, I give you fair warning,\" either you or your head must be off, and that in about half no time! Take your choice!\"\n",
                        "*   “Leave off that!”\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Initialize Gemini client\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "\n",
                "load_dotenv()\n",
                "client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))\n",
                "\n",
                "def semantic_retrieval(query, index, chunks, top_k=3):\n",
                "    \"\"\"Retrieve relevant chunks using semantic similarity\"\"\"\n",
                "    # Encode query\n",
                "    query_embedding = embedding_model.encode([query])\n",
                "    \n",
                "    # Search FAISS index\n",
                "    distances, indices = index.search(query_embedding.astype(np.float32), top_k)\n",
                "    print(f\"Indices: {indices[0]} {distances[0]}\")\n",
                "    \n",
                "    # Return sorted chunks by relevance\n",
                "    return [chunks[i] for i in indices[0]]\n",
                "\n",
                "# Sample query\n",
                "query = \"what did the queen shout at the top of her voice\"\n",
                "\n",
                "# Retrieve relevant context\n",
                "context_chunks = semantic_retrieval(query, index, chunks)\n",
                "context = \"\\n\".join(context_chunks)\n",
                "\n",
                "# Generate response\n",
                "response = client.models.generate_content(\n",
                "    model=\"gemini-2.0-flash\",\n",
                "    contents=f\"\"\"Answer the question based on the following context:\n",
                "    {context}\n",
                "    \n",
                "    Question: {query}\n",
                "    Answer:\"\"\"\n",
                ")\n",
                "\n",
                "# Display results\n",
                "print(\"Retrieved Context:\")\n",
                "for chunk in context_chunks:\n",
                "    print(f\"- {chunk}...\")\n",
                "\n",
                "print(\"\\nGenerated Answer:\")\n",
                "print(response.text)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary-section",
            "metadata": {},
            "source": [
                "## Key Enhancements\n",
                "\n",
                "1. **Semantic Embeddings**: Uses `all-MiniLM-L6-v2` model for dense vector representations\n",
                "2. **FAISS Index**: Efficient similarity search for quick retrieval\n",
                "3. **Contextual Understanding**: Better captures semantic relationships than keyword matching\n",
                "4. **Scalability**: Can handle larger document collections efficiently\n",
                "\n",
                "To further improve:\n",
                "- Experiment with different embedding models\n",
                "- Add metadata filtering\n",
                "- Implement hybrid search (dense + sparse)\n",
                "- Use more sophisticated chunking strategies"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
