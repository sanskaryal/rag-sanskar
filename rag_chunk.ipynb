{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro-cell",
            "metadata": {},
            "source": [
                "# Enhanced RAG System with Hybrid Search and Metadata\n",
                "\n",
                "This notebook demonstrates improvements to our RAG system. The changes include:\n",
                "\n",
                "- A more reliable embedding model (`all-mpnet-base-v2`)\n",
                "- A sophisticated, sentence-based text chunking strategy that attaches metadata\n",
                "- A sparse (TF-IDF) index for keyword search\n",
                "- A hybrid search combining dense (FAISS) and sparse (TF-IDF) scoring\n",
                "- Optional metadata filtering of chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "imports-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import os\n",
                "import numpy as np\n",
                "import faiss\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# Use a more powerful embedding model\n",
                "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
                "\n",
                "from google import genai\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "functions-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "def sophisticated_chunk_text(text, min_sentences=3, overlap=1):\n",
                "    \"\"\"\n",
                "    Splits text into chunks using sentence tokenization. Each chunk contains at least\n",
                "    min_sentences, and overlaps with the previous chunk by the last 'overlap' sentences.\n",
                "    Returns a list of dictionaries with 'chunk' and 'metadata'.\n",
                "    \"\"\"\n",
                "    sentences = sent_tokenize(text)\n",
                "    chunks = []\n",
                "    i = 0\n",
                "    while i < len(sentences):\n",
                "        chunk_sentences = sentences[i:i+min_sentences]\n",
                "        if not chunk_sentences:\n",
                "            break\n",
                "        chunk_text = \" \".join(chunk_sentences)\n",
                "        metadata = {\"start_sentence\": i, \"end_sentence\": i + len(chunk_sentences) - 1}\n",
                "        chunks.append({\"chunk\": chunk_text, \"metadata\": metadata})\n",
                "        # Move forward by min_sentences - overlap sentences\n",
                "        i += max(min_sentences - overlap, 1)\n",
                "    return chunks\n",
                "\n",
                "def create_faiss_index(embeddings):\n",
                "    \"\"\"\n",
                "    Create a FAISS index for dense vector retrieval from the provided embeddings.\n",
                "    \"\"\"\n",
                "    dimension = embeddings.shape[1]\n",
                "    index = faiss.IndexFlatL2(dimension)\n",
                "    index.add(embeddings.astype(np.float32))\n",
                "    return index\n",
                "\n",
                "def create_sparse_index(chunks):\n",
                "    \"\"\"\n",
                "    Create a sparse TF-IDF index from the list of chunk dictionaries.\n",
                "    \"\"\"\n",
                "    documents = [item[\"chunk\"] for item in chunks]\n",
                "    vectorizer = TfidfVectorizer()\n",
                "    sparse_matrix = vectorizer.fit_transform(documents)\n",
                "    return vectorizer, sparse_matrix\n",
                "\n",
                "def hybrid_search(query, dense_index, sparse_vectorizer, sparse_matrix, chunks, top_k=3, alpha=0.5):\n",
                "    \"\"\"\n",
                "    Retrieve relevant chunks using both dense (FAISS) and sparse (TF-IDF) search.\n",
                "    The hybrid score is computed as a weighted sum of:\n",
                "      - Dense score: inverse of the Euclidean distance\n",
                "      - Sparse score: TF-IDF cosine similarity score\n",
                "    alpha weights the dense score (0 <= alpha <= 1).\n",
                "    \"\"\"\n",
                "    # Dense search\n",
                "    query_embedding = embedding_model.encode([query])\n",
                "    distances, dense_indices = dense_index.search(query_embedding.astype(np.float32), top_k)\n",
                "\n",
                "    # Sparse search\n",
                "    query_sparse = sparse_vectorizer.transform([query])\n",
                "    sparse_scores = (sparse_matrix * query_sparse.T).toarray().ravel()\n",
                "    sparse_indices = np.argsort(-sparse_scores)[:top_k]\n",
                "\n",
                "    # Combine candidate indices from both searches\n",
                "    combined_indices = set(dense_indices[0]).union(set(sparse_indices))\n",
                "    combined_scores = {}\n",
                "\n",
                "    # Compute a combined score for each candidate\n",
                "    for idx in combined_indices:\n",
                "        # For dense score, approximate by inverse Euclidean distance\n",
                "        chunk_embedding = embedding_model.encode([chunks[idx]['chunk']])\n",
                "        dense_score = 1 / (1 + np.linalg.norm(query_embedding - chunk_embedding))\n",
                "        sparse_score = sparse_scores[idx]\n",
                "        combined_scores[idx] = alpha * dense_score + (1 - alpha) * sparse_score\n",
                "\n",
                "    # Rank candidates by the combined score\n",
                "    ranked = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
                "    return [chunks[idx]['chunk'] for idx, _ in ranked]\n",
                "\n",
                "def filter_chunks_by_metadata(chunks, condition):\n",
                "    \"\"\"\n",
                "    Filter chunks based on a metadata condition. The condition is a function\n",
                "    that takes a metadata dictionary and returns True or False.\n",
                "    \"\"\"\n",
                "    return [item for item in chunks if condition(item[\"metadata\"])]\n",
                "\n",
                "def semantic_retrieval_hybrid(query, dense_index, sparse_vectorizer, sparse_matrix, chunks, top_k=3):\n",
                "    \"\"\"\n",
                "    Use the hybrid search for retrieval.\n",
                "    \"\"\"\n",
                "    return hybrid_search(query, dense_index, sparse_vectorizer, sparse_matrix, chunks, top_k)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "document-cell",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt_tab to\n",
                        "[nltk_data]     C:\\Users\\myhom\\AppData\\Roaming\\nltk_data...\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total chunks created: 373\n",
                        "Example chunk: {'chunk': \"Alice’s Adventures in Wonderland | Project Gutenberg The Project Gutenberg eBook of Alice's Adventures in Wonderland This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org . If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. Title : Alice's Adventures in Wonderland Author : Lewis Carroll Release date : June 27, 2008 [eBook #11] Most recently updated: November 10, 2024 Language : English Credits : Arthur DiBianca and David Widger *** START OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND *** Alice’s Adventures in Wonderland by Lewis Carroll THE MILLENNIUM FULCRUM EDITION 3.0 Contents CHAPTER I. Down the Rabbit-Hole CHAPTER II.\", 'metadata': {'start_sentence': 0, 'end_sentence': 4}}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
                    ]
                }
            ],
            "source": [
                "# For demonstration, we use a sample document.\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "\n",
                "# Fetch a sample webpage (Alice's Adventures in Wonderland from Gutenberg)\n",
                "url = \"https://www.gutenberg.org/cache/epub/11/pg11-images.html\"\n",
                "response = requests.get(url)\n",
                "html_content = response.text\n",
                "\n",
                "soup = BeautifulSoup(html_content, 'html.parser')\n",
                "for script in soup([\"script\", \"style\"]):\n",
                "    script.decompose()\n",
                "\n",
                "document = soup.get_text(separator=\" \", strip=True)\n",
                "clean_text = re.sub('\\s+', ' ', document).strip()\n",
                "nltk.download('punkt_tab')\n",
                "# Create chunks using the sophisticated sentence-based function\n",
                "chunks = sophisticated_chunk_text(clean_text, min_sentences=5, overlap=2)\n",
                "print(f\"Total chunks created: {len(chunks)}\")\n",
                "print(\"Example chunk:\", chunks[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "index-cell",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dense index built with 373 items\n"
                    ]
                }
            ],
            "source": [
                "# Build dense and sparse indexes from the chunks\n",
                "chunk_texts = [item[\"chunk\"] for item in chunks]\n",
                "\n",
                "# Generate dense embeddings\n",
                "chunk_embeddings = embedding_model.encode(chunk_texts)\n",
                "dense_index = create_faiss_index(chunk_embeddings)\n",
                "\n",
                "# Build the sparse (TF-IDF) index\n",
                "sparse_vectorizer, sparse_matrix = create_sparse_index(chunks)\n",
                "print(f\"Dense index built with {chunk_embeddings.shape[0]} items\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "query-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a sample query\n",
                "query = \"what did the queen shouted at the top of her voice\"\n",
                "\n",
                "# Optionally filter chunks by metadata, for example, only consider chunks starting after sentence 50\n",
                "filtered_chunks = filter_chunks_by_metadata(chunks, lambda m: m[\"start_sentence\"] > 50)\n",
                "\n",
                "# if filtered_chunks and len(filtered_chunks) < len(chunks):\n",
                "#     print(f\"Using metadata-filtered chunks: {len(filtered_chunks)} items\")\n",
                "#     filtered_texts = [item[\"chunk\"] for item in filtered_chunks]\n",
                "#     filtered_embeddings = embedding_model.encode(filtered_texts)\n",
                "#     dense_index = create_faiss_index(filtered_embeddings)\n",
                "#     sparse_vectorizer, sparse_matrix = create_sparse_index(filtered_chunks)\n",
                "#     results = semantic_retrieval_hybrid(query, dense_index, sparse_vectorizer, sparse_matrix, filtered_chunks)\n",
                "# else:\n",
                "results = semantic_retrieval_hybrid(query, dense_index, sparse_vectorizer, sparse_matrix, chunks)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "58fd93c4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Retrieved Chunks:\n",
                        "- “Nothing whatever? ” persisted the King. “Nothing whatever,” said Alice. “That’s very important,” the King said, turning to the jury. They were just beginning to write this down on their slates, when  ...\n",
                        "- “I want a clean cup,” interrupted the Hatter: “let’s all move one place on.” He moved on as he spoke, and the Dormouse followed him: the March Hare moved into the Dormouse’s place, and Alice rather un ...\n",
                        "- However, he consented to go on. “And so these three little sisters—they were learning to draw, you know—” “What did they draw?” said Alice, quite forgetting her promise. “Treacle,” said the Dormouse,  ...\n",
                        "\n",
                        "Generated Answer:\n",
                        "The provided text does not include any mention of the Queen shouting. The characters in the text include the King, Alice, the White Rabbit, the Hatter, the Dormouse, and the March Hare.\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "print(\"\\nRetrieved Chunks:\")\n",
                "for res in results:\n",
                "    print(\"-\", res[:200], \"...\")\n",
                "\n",
                "# Generate an answer using Gemini LLM\n",
                "context = \"\\n\".join(results)\n",
                "prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
                "\n",
                "response = client.models.generate_content(\n",
                "    model=\"gemini-2.0-flash\",\n",
                "    contents=prompt\n",
                ")\n",
                "\n",
                "print(\"\\nGenerated Answer:\")\n",
                "print(response.text)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
